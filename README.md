# 30Music-artist-recommendation

## Dataset
[30Music Dataset](http://recsys.deib.polimi.it/datasets/)

## Repository organisation

* Data preparation: [data_preparation.ipynb](data_preparation.ipynb)

* Baseline model - collaborative filtering for implicit feedback datasets: [collaborative_filtering.ipynb](collaborative_filtering.ipynb)

* Stronger model - item2vec embeddings: [item2vec.ipynb](item2vec.ipynb)

* Prepared data for training, validation and testing: [data](data/)

* Helper functions used in the notebooks: [utils](utils/)


## Process description
1. Я подготовила данные: перешла от истории прослушиваний треков к истории прослушиваний исполнителей. Процесс можно посмотреть в ноутбуке 
[data_preparation.ipynb](data_preparation.ipynb).

2. Я решила разбить данные на Train и Test по пользователям - 80% пользователей я оставила в трейне и остальные в тестовом датасете. 
Я разделила данные таким образом, потому что было бы хорошо построить модель, способную рекоммендовать испольнителей новым юзерам, то есть тем, кого не было в трейнинг сете (с этим есть проблемы у коллаборативной фильтрации). В качестве сета для валидации я решила использовать hit rate и left-one-out технику. Для каждого юзера из трейнинг сета я удалила по одному исполнителю, с которым этот юзер взаимодействовал (слушал его треки), и составила из этих пар юзер-исполнитель left-one-out сет. Таким образом, я пыталась рекоммендовать топ-20 лучших исполнителей и если среди этих 20 для каждого юзера был тот, которого я отбросила, я считала это за hit. Поделив на количество юзеров, получала hit rate. То есть, я использовала hit rate скор, чтобы выбрать лучшую модель при тьюнинге гиперпараметров, а потом делала предсказания на тест сете, используя модель с лучшими параметрами. Затем я делала эвалюэйшн уже на тест сете - считала precision, recall и MAPk.


3. В качестве бейзлайн модели я выбрала **коллаборативную фильтрацию**. Я выбрала этот метод, так как это один из самых популярных методов для построения рекоммендательных систем. Он дает неплохие результаты, и натренировать модель почти не занимает времени. Так как задача была найти векторные представления для исполнителей, такие, чтобы можно было бы находить похожих исполнителей, например, с помощью кластеризации, я применяла коллаборативную фильтрацию, чтобы извлечь item factors и посмотреть, насколько они применимы для того, чтобы искать похожих исполнителей. 

В данном случае, у нас нет рейтингов исполнителей и тд., то есть это данные типа implicit feedback. Я применяла метод, описанный в статье [Collaborative Filtering for Implicit Feedback Datasets](http://yifanhu.net/PUB/cf.pdf). Этот метод реализован в библиотеке [implicit](https://github.com/benfred/implicit), которая очень удобна в использовании и имеет хорошую документацию. Чтобы получить векторные представления исполнителей, я составила матрицу user-artist в которой значением на пересечении строки юзера и столбца исполнителя является количество прослушиваний данного исполнителя юзером. Я применила Alterating Least Squares, чтобы получить разложение матрицы. Таким образом, я получила 2 матрицы -- вектора для юзеров и исполнителей. 

Я натренировала 3 CF модели с разными параметрами, чтобы для каждой посчитать hit rate и тем самым оценить, какая из них лучше. Для всех моделей hit rate оказался нулевым. Это связано с тем, как работает матричное разложение. Скорее всего в топе оказывались те исполнители, с которыми юзер контактировал согласно трейнинг сету, а новые исполнители оказывались в конце. Возможно надо было переписать функцию вычисления hit rate так, чтобы она оченивала те топ-20 рекомендованных исполнителей, с которым пользователь не контактировал в трейнинг сете. Но не хватило времени. 





**Что хотелось бы доделать?**





## Top-20 similar articts: CF vs item2vec


